<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> HDFS: How a file is written! · Codailama</title><meta name="description" content="HDFS: How a file is written! - Santosh Pingale"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://codailama.com/atom.xml" title="Codailama"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://in.linkedin.com/in/santoshpingale" target="_blank" class="nav-list-link">LINKEDIN</a></li><li class="nav-list-item"><a href="https://github.com/santosh-d3vpl3x" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">HDFS: How a file is written!</h1><div class="post-info">Mar 4, 2015</div><div class="post-content"><p>HDFS is distributed file system capable of storing very large files without much effort. HDFS spans across multiple machines on multiple racks. There are two types of nodes in HDFS:</p>
<ul>
<li>DataNode</li>
<li>NameNode</li>
</ul>
<p>The nodes responsible for storing files and handling IOs are DataNodes where as NameNode is responsible for keeping the fs image and upkeep of the HDFS. HDFS is robust, highly flexible, faster, fault&#xA0;tolerant,&#xA0;easier to maintain, easier to scale out and reliable. Various decisions made while designing HDFS are responsible for such features and it makes HDFS suitable for many usecases.&#xA0; Lets have a brief look at how HDFS is designed and how file is written internally.<br><a id="more"></a></p>
<div class="toc">

<!-- toc -->
<ul>
<li><a href="#writing-a-file-to-hdfs">Writing a file to HDFS</a><ul>
<li><a href="#from-low-earth-orbit">From Low Earth Orbit:</a></li>
<li><a href="#from-36000-feet">From 36000 feet:</a></li>
<li><a href="#from-5000-feet">From 5000 feet:</a></li>
</ul>
</li>
<li><a href="#references">References:</a></li>
</ul>
<!-- tocstop -->
</div>

<p><a href="http://130.211.135.58/wp-content/uploads/2015/03/HDFS-topology.png" target="_blank" rel="external"><img src="http://130.211.135.58/wp-content/uploads/2015/03/HDFS-topology.png" alt=""></a></p>
<p>HDFS provides a very easy API for interacting with the files stored in it. HDFS can be queried from command line, code and over web. To add sugar coating, its file system commands resemble linux file system commands and it also hide away the internal complexities of distributed file system. For example, if you want to read a file from HDFS then execute <code>hadoop fs -cat /path/to/file/on/hdfs</code> which is very similar to <code>cat /path/to/file</code> in linux. A single file is distributed across cluster and can be accessed with the help of API. HDFS takes most of the pain while interacting with the client. The same thing happens when you push a file to HDFS.&#xA0;</p>
<p>Essentially, all IO commands issued to HDFS inolves multiple steps and they are highly optimized for maximum throughput and reliability. While writing a file to HDFS, one can simply exeute <code>hadoop fs -put /path/to/file/on/local /path/to/file/on/hdfs</code> and HDFS will make sure the file is written to multiple nodes. HDFS also maintains metadata of these blocks so that files can be read from it.&#xA0;</p>
<p>Have you ever wondered, what really goes on behind the scene while you execute these commands? Well, whatever happens HDFS shouldn&#x2019;t stay in HDFS and we should definitely know about it. Lets dive deep to see how a file is written on HDFS.</p>
<h2 id="writing-a-file-to-hdfs"><a href="#Writing-a-file-to-HDFS" class="headerlink" title="Writing a file to HDFS"></a>Writing a file to HDFS <a href="#writing-a-file-to-hdfs" class="header-anchor">#</a></h2><p>Lets have a look at how file can be written to HDFS with the help of command line tool. You can choose to write the file using web API, programmatically or through command line but the internals will remain exactly the same.&#xA0;</p>
<h3 id="from-low-earth-orbit"><a href="#From-Low-Earth-Orbit" class="headerlink" title="From Low Earth Orbit:"></a>From Low Earth Orbit: <a href="#from-low-earth-orbit" class="header-anchor">#</a></h3><p>To write a file to HDFS one should execute&#xA0;<code>-copyFromLocal</code>, <code>-put</code>, <code>-appendToFile</code>&#xA0;with appropriate arguments and options. Done! File is split into blocks and then those blocks are placed onto HDFS on different DataNodes.</p>
<h3 id="from-36000-feet"><a href="#From-36000-feet" class="headerlink" title="From 36000 feet:"></a>From 36000 feet: <a href="#from-36000-feet" class="header-anchor">#</a></h3><p>When we execute one of these commands, client should know to which namespace it needs to write the file. It means, it need to know the NameNode seating on top of that namespace. Client connects to NameNode, get the DataNode addresses which are going to host the files and transfers the files on those DataNodes.</p>
<p>If there were five blocks and replication factor of three, then HDFS with five DataNodes will store blocks such that unavailability of any of the node won&#x2019;t cause any data loss.<br><a href="http://130.211.135.58/wp-content/uploads/2015/03/replication-no-policy.png" target="_blank" rel="external"><img src="http://130.211.135.58/wp-content/uploads/2015/03/replication-no-policy.png" alt=""></a></p>
<p>Even if DataNode 2 Goes down, the files are still available on other nodes. Hence we still have data and then HDFS can re-replicate the lost blocks.</p>
<p><a href="http://130.211.135.58/wp-content/uploads/2015/03/replication-no-policy-data-loss.png" target="_blank" rel="external"><img src="http://130.211.135.58/wp-content/uploads/2015/03/replication-no-policy-data-loss.png" alt=""></a><br>Replication also helps us increase data locality. It also helps in MapReduce principle which says &#x201C;Process should travel to data as it is cheap and not the other way around&#x201D;.</p>
<h3 id="from-5000-feet"><a href="#From-5000-feet" class="headerlink" title="From 5000 feet:"></a>From 5000 feet: <a href="#from-5000-feet" class="header-anchor">#</a></h3><p>Well, the data has been broken to blocks, written to multiple nodes, it has been replicated and now we can start reading it from HDFS, This raises few questions!</p>
<ol>
<li>Who decides the replication factor and block size?</li>
<li>Who breaks the file to blocks?</li>
<li>How data is written to DataNodes and essentially, how it is replicated across cluster?</li>
<li>How replicas are placed, how DataNodes are chosen for given file block?</li>
<li>How optimized utilization of resources is ensured?</li>
</ol>
<p>Lets drill down a bit for answering these questions.</p>
<h4 id="replication-factor-and-block-size"><a href="#Replication-Factor-and-Block-Size" class="headerlink" title="Replication Factor and Block Size:"></a>Replication Factor and Block Size: <a href="#replication-factor-and-block-size" class="header-anchor">#</a></h4><p>By default, HDFS has a replication factor of 3 and block size of 64 Mb. These properties can be changed at the time of setup. <code>dfs.replication</code> and <code>fs.block.size</code> from <code>hdfs-site.xml</code>&#xA0;are responsible for these properties. If cluster is already operational and you decide to change these properties afterwards, existing blocks on the cluster will remain unchanged and only affects newly added blocks to the cluster.<br>Client receives these settings from <del datetime="2015-05-07T13:15:48+00:00">NameNode</del> DataNode. These are file-level settings implying client has ability to override these properties per file basis. Client then breaks file into blocks as it starts process to store it in HDFS.</p>
<h4 id="writing-data"><a href="#Writing-Data" class="headerlink" title="Writing Data:"></a>Writing Data: <a href="#writing-data" class="header-anchor">#</a></h4><p>After getting all configurations, client asks NameNode for list of DataNodes on which it should host the blocks. Blocks are sequentially written to HDFS and for each block NameNode provides list.<br>The flow diagram explains the process.</p>
<p><a href="http://130.211.135.58/wp-content/uploads/2015/03/WriteFile.png" target="_blank" rel="external"><img src="http://130.211.135.58/wp-content/uploads/2015/03/WriteFile.png" alt=""></a></p>
<p>HDFS forms a <em>replication pipeline</em> when data flow starts from client. This replication pipeline ensures that the needed replication factor is met.<br>In the flow diagram above, client starts streaming packets to the <code>DN-1</code>. <code>DN-1</code> in turn transfers these packets to other DataNode and then that data node transfers the data to subsequent DataNode if necessary. These DataNodes start transferring packet as soon as they receive a data packet completely. Data packet is buffered at each hop in the pipeline. The client do not wait for acknowledgement from these DataNodes and starts to transmit another packet irrespective of status of previous packet. Client also sends checksum for each block written to the HDFS which helps in ensuring the integrity of the data.<br>Once the data block has been streamed completely to the DataNode, that DataNode sends report to NameNode.<br>Then this process is repeated for another block in the file. All the blocks have same size except for last block. When all blocks have been written to HDFS, NameNode finalizes the metadata and updates the journal file.<br>I have left out concept of <em>lease</em>, <code>hflush</code>/<code>hsync</code> and <code>close</code>,<em>Write Once Read Multiple</em> concept, <em>checkpoint</em>, <em>journal</em>, <em>fsimage</em> as these are not in the scope of this article.&#xA0;</p>
<h4 id="choosing-datanodes"><a href="#Choosing-DataNodes" class="headerlink" title="Choosing DataNodes:"></a>Choosing DataNodes: <a href="#choosing-datanodes" class="header-anchor">#</a></h4><p>HDFS is rack aware and forms a network topology based on rack along with DataNode configurations. Once DataNodes have been associated with a particular rack, HDFS can then use its algorithm to deduce on which node the data should be stored.&#xA0;</p>
<p>Lets see how this algorithm works:<br>HDFS calculates network topology distance of each node from writer client. When a new block is created, it places the first replica on the node where the writer is located. The second and the third replicas are placed on two different nodes in a different rack. The rest are placed on random nodes with restrictions that no more than one replica is placed at any one node and no more than two replicas are placed in the same rack, if possible. The choice to place the second and third replicas on a different rack better distributes the block replicas for a single file across the cluster. If the first two replicas were placed on the same rack, for any file, two-thirds of its block replicas would be on the same rack.</p>
<p>After all target nodes are selected, nodes are organized as a pipeline in the order of their proximity to the first replica. Data are pushed to nodes in this order. For reading, the NameNode first checks if the client&#x2019;s host is located in the cluster. If yes, block locations are returned to the client in the order of its closeness to the reader. The block is read from DataNodes in this preference order.</p>
<p>This policy reduces the inter-rack and inter-node write traffic and generally improves write performance. Because the chance of a rack failure is far less than that of a node failure, this policy does not impact data reliability and availability guarantees. In the usual case of three replicas, it can reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three.<br><strong>Algorithm takes following parameters in consideration</strong>:</p>
<ol>
<li>Whether selected node is free, if it is not then chose another.</li>
<li>If given node has free space left.</li>
<li>Health of the given node.</li>
</ol>
<p><strong>Algorithm does not take into consideration</strong>:</p>
<ol>
<li>If data transfer is going to keep size of HDFS uniform across nodes i.e. balanced.</li>
<li>Network bandwidth between two nodes.</li>
</ol>
<p><strong>Algorithm ensures</strong>:</p>
<ol>
<li>Data is kept onto multiple racks as per rack placement policy.</li>
<li>Minimizes the write cost.</li>
<li>Maximizes data reliability, availability and aggregate read bandwidth.</li>
</ol>
<p>HDFS carries arsenal of tools with it to ensure data is always available in required form. Be it block scanner, balancer, replication manager or extra NameNodes serving as CheckpointNode/BackupNode, everything is very well orchestrated and hence HDFS is the most adapted file system for Hadoop.</p>
<h2 id="references"><a href="#References" class="headerlink" title="References:"></a>References: <a href="#references" class="header-anchor">#</a></h2><p><a href="http://www.aosabook.org/en/hdfs.html" target="_blank" rel="external">http://www.aosabook.org/en/hdfs.html</a><br><a href="https://hadoop.apache.org/docs/r1.1.1/api/org/apache/hadoop/net/NetworkTopology.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r1.1.1/api/org/apache/hadoop/net/NetworkTopology.html</a><br><a href="https://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a><br><a href="http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf" target="_blank" rel="external">http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf</a></p>
</div></article></div></section><footer><div class="paginator"><a href="/2015/03/04/issuefix-too-many-hive-staging/" class="prev">PREV</a><a href="/2015/02/22/javascript-equality-truthyness/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'codefudge';
var disqus_identifier = '2015/03/04/hdfs-how-file-is-written/';
var disqus_title = 'HDFS: How a file is written!';
var disqus_url = 'http://codailama.com/2015/03/04/hdfs-how-file-is-written/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//codefudge.disqus.com/count.js" async></script><div class="copyright"><p>© 2013 - 2016 <a href="http://codailama.com">Santosh Pingale</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-6516328-4",'auto');ga('send','pageview');</script></body></html>